# -*- coding: utf-8 -*-
"""System_Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15-Usms12xw2R0z1j_EFZIaXDX0k5MfFZ
"""

!unzip '/content/drive/MyDrive/File Submission/Book_Recomendations.zip' -d Book_Recomendations

"""1. Import Library"""

import pandas as pd              # Untuk manipulasi data
import numpy as np               # Untuk operasi numerik
import matplotlib.pyplot as plt  # Untuk visualisasi (optional)
import seaborn as sns            # Visualisasi data (optional)
from sklearn.feature_extraction.text import TfidfVectorizer  # NLP - vectorizer
from sklearn.metrics.pairwise import cosine_similarity       # Hitung kemiripan antar vektor
import re                          # Untuk preprocessing teks (regex)
import string                      # Untuk hapus tanda baca
from sklearn.metrics.pairwise import linear_kernel
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy
from sklearn.neighbors import NearestNeighbors

"""2. Load semua dari dataset"""

df_book = pd.read_csv('/content/Book_Recomendations/Books.csv', low_memory=False)

df_Ratings = pd.read_csv('/content/Book_Recomendations/Ratings.csv')

df_Users = pd.read_csv('/content/Book_Recomendations/Users.csv')

"""# **Data Understanding**
### BOOKS
Menampilkan 5 baris pertama dari dataset
"""

df_book.head()

"""Menampilkan Jumlah baris dan kolom pada dataset Books.csv"""

print("Number of rows:", df_book.shape[0])
print("Number of columns:", df_book.shape[1])

"""Melihat Fitur yang ada pada dataset Books.csv
check tipe dari setiap data, missing value dan data duplikat.

ditemukan missing value pada:

Book-Author :           2

Publisher :             2

Image-URL-L   :         3
"""

# Display all available features (columns)
print("Available Features:\n", df_book.columns.tolist())

# Check data types of each feature
print("\nData Types:\n", df_book.dtypes)

# Check for missing values
print("\nMissing Values:\n", df_book.isnull().sum())

# Check for duplicate rows
print("\nNumber of Duplicate Rows:", df_book.duplicated().sum())

"""Melihat apakah ada outlier pada data Year-Of-Publication

Terdapat Outlier dimana buku di publish melebihi waktu sekarang dan juga sebelum tahun 1980, seperti tahun 0 atau sebelum 1500.
"""

# Calculate quartiles (Q1, Q3) and IQR
Q1 = df_book['Year-Of-Publication'].quantile(0.25)
Q3 = df_book['Year-Of-Publication'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Count outliers
outliers_low = df_book[(df_book['Year-Of-Publication'] < lower_bound)]
outliers_high = df_book[(df_book['Year-Of-Publication'] > upper_bound)]
total_outliers = len(outliers_low) + len(outliers_high)


print(f"Number of low outliers: {len(outliers_low)}")
print(f"Number of high outliers: {len(outliers_high)}")
print(f"Total number of outliers: {total_outliers}")

# Melihat apakah ada outlier pada data Year-Of-Publication
plt.figure(figsize=(10, 6))
sns.boxplot(x='Year-Of-Publication', data=df_book)
plt.title('Box Plot of Year-Of-Publication')
plt.show()

# Alternative method using describe() to identify potential outliers
print(df_book['Year-Of-Publication'].describe())

"""### **USER**

Dilakukan pengecekan juga ada pada Dataset User.
"""

df_Users.head()

"""Melihat jumlah baris data dan kolom pada dataset Users"""

print("Number of rows:", df_Users.shape[0])
print("Number of columns:", df_Users.shape[1])

"""Melihat Fitur, tipe data, missing value, dan duplikat data.

dimana disini ditemukan 110762 missing values pada data Age, yang hal ini sering ditemukan karena beberapa orang memilih tidak memberitahukan umurnya.
"""

# Display all available features (columns)
print("Available Features:\n", df_Users.columns.tolist())

# Check data types of each feature
print("\nData Types:\n", df_Users.dtypes)

# Check for missing values
print("\nMissing Values:\n", df_Users.isnull().sum())

# Check for duplicate rows
print("\nNumber of Duplicate Rows:", df_Users.duplicated().sum())

"""Melihat apakah ada Outlier pada data Age.

Disini dapat dilihat terdapat beberapa Outlier (Umur yang tidak wajar) dari min 0 dan nilai Max 244.
"""

# Check for outliers in 'Age' column of df_Users
print("\nAge Statistics:\n", df_Users['Age'].describe())

# Visualize the distribution of 'Age'
plt.figure(figsize=(10, 6))
sns.boxplot(x=df_Users['Age'])
plt.title('Boxplot of User Ages')
plt.show()

"""melihat data yang dinilai Outlier dengan IQR method"""

# Identify outliers using IQR method
Q1 = df_Users['Age'].quantile(0.25)
Q3 = df_Users['Age'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df_Users[(df_Users['Age'] < lower_bound) | (df_Users['Age'] > upper_bound)]
print("\nNumber of outliers in 'Age':", len(outliers))
print("\nOutliers:\n", outliers)

"""### **RATING**

dilakukan pengecekan yang sama pada data Rating
"""

df_Ratings.head()

"""Melihat Jumlah bari dan kolom pada dataset Ratings"""

print("Number of rows:", df_Ratings.shape[0])
print("Number of columns:", df_Ratings.shape[1])

"""Melihat semua Fitur pada dataset, Jenis tipe data, missing value dan Duplikat. yang dimana tidak ditemukan disini."""

# Display all available features (columns)
print("Available Features:\n", df_Ratings.columns.tolist())

# Check data types of each feature
print("\nData Types:\n", df_Ratings.dtypes)

# Check for missing values
print("\nMissing Values:\n", df_Ratings.isnull().sum())

# Check for duplicate rows
print("\nNumber of Duplicate Rows:", df_Ratings.duplicated().sum())

"""# **Exploratory Data Analyst (EDA)**

Melihat Publisher yang mempublish buku terbanyak pada dataset ini, yaitu :

Harlequin	7535

Silhouette	4220

Pocket	3905

Ballantine Books	3783

Bantam Books
"""

df_book['Publisher'].value_counts()

"""Melihat Author yang memiliki buku terbanyak pada dataset ini, yaitu :

Agatha Christie	632

William Shakespeare	567

Stephen King	524

Ann M. Martin	423

Carolyn Keene	373
"""

df_book['Book-Author'].value_counts()

"""Melihat Distribusi dari Ratings.

dimana disini Rating 0 adalah tidak memberikan penilaian terhadap bukunya.
"""

# Check the distribution of ratings
print(df_Ratings['Book-Rating'].describe())

# Visualize the distribution of ratings
plt.figure(figsize=(10, 6))
sns.countplot(x='Book-Rating', data=df_Ratings)
plt.title('Distribution of Book Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

"""Melihat Buku yang paling banyak di Rating oleh User, yaitu :

Wild Animus        2502.0

The Lovely Bones: A Novel        1295.0

The Da Vinci Code         883.0

Divine Secrets of the Ya-Ya Sisterhood: A Novel         732.0

The Red Tent (Bestselling Backlist)         723.0
"""

# Calculate the popularity of books based on the number of ratings
book_popularity = df_Ratings.groupby('ISBN')['Book-Rating'].count().reset_index()
book_popularity.rename(columns={'Book-Rating': 'Rating_Count'}, inplace=True)

# Merge popularity data with book information
df_merged = pd.merge(df_book, book_popularity, on='ISBN', how='left')

# Sort books by rating count in descending order and get the top 10
top_10_books = df_merged.sort_values(by='Rating_Count', ascending=False).head(10)

# Display the top 10 books with their titles and rating counts
print(top_10_books[['Book-Title', 'Rating_Count']])

"""User teraktif yang memberikan Rating, yaitu user dengan Id 11676"""

top_active_user = df_Ratings['User-ID'].value_counts().idxmax()
print(f"The most active user is: {top_active_user}")

"""Melihat Negara dengan Jumlah User terbanyak, dimana disini dapat dilihat USA menepati nomor datu, kemudian Canada dan setelahnya UK."""

# Pisahkan kolom location menjadi kota, negara bagian, negara
df_Users[['city', 'state', 'country']] = df_Users['Location'].str.split(',', expand=True, n=2)
df_Users['country'] = df_Users['country'].str.strip()  # Hilangkan spasi

# Hitung jumlah user per negara
top_countries = df_Users['country'].value_counts().head(10)

# Plot top 10 negara dengan user terbanyak
plt.figure(figsize=(10,6))
sns.barplot(x=top_countries.values, y=top_countries.index, palette='viridis')
plt.title('Top 10 Negara dengan Jumlah User Terbanyak')
plt.xlabel('Jumlah User')
plt.ylabel('Negara')
plt.tight_layout()
plt.show()

"""Disini diperlihatkan untuk Distribusi Usia pengguna.

"""

# Filter usia valid (range 5 - 100 tahun)
df_users_clean = df_Users[(df_Users['Age'] >= 5) & (df_Users['Age'] <= 100)]

# Histogram distribusi usia
plt.figure(figsize=(10,6))
sns.histplot(df_users_clean['Age'], bins=20, kde=True, color='skyblue')
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Usia')
plt.ylabel('Jumlah Pengguna')
plt.tight_layout()
plt.show()

# Tambahan: Plot distribusi kelompok usia
bins = [0, 10, 20, 30, 40, 50, 60, 70, 100]
labels = ['0-10','11-20','21-30','31-40','41-50','51-60','61-70','71+']
df_users_clean['AgeGroup'] = pd.cut(df_users_clean['Age'], bins=bins, labels=labels, right=False)

age_group_counts = df_users_clean['AgeGroup'].value_counts().sort_index()

plt.figure(figsize=(10,6))
sns.barplot(x=age_group_counts.index, y=age_group_counts.values, palette='coolwarm')
plt.title('Distribusi Pengguna Berdasarkan Kelompok Usia')
plt.xlabel('Kelompok Usia')
plt.ylabel('Jumlah Pengguna')
plt.tight_layout()
plt.show()

"""# **Data Preparation**

Penjelasan: Mengubah kolom tahun publikasi menjadi tipe numerik. Jika ada nilai tidak valid (misalnya teks), akan diubah jadi NaN.

Tujuan: Supaya bisa dianalisis atau difilter sebagai angka, bukan string.
"""

df_book['Year-Of-Publication'] = pd.to_numeric(df_book['Year-Of-Publication'], errors='coerce')
print(df_book['Year-Of-Publication'].dtype)

"""Penjelasan: Menghapus data buku dengan tahun publikasi yang tidak wajar (di luar 1980–2024).

Tujuan: Menghindari data outlier yang bisa mengganggu analisis atau rekomendasi (misalnya tahun 0 atau 9999).
"""

df_book = df_book[(df_book['Year-Of-Publication'] >= 1980) & (df_book['Year-Of-Publication'] <= 2024)]
print(df_book['Year-Of-Publication'].describe())

"""Penjelasan: Menghapus baris yang memiliki nilai kosong di kolom penting seperti penulis, penerbit, dan gambar.

Tujuan: Kolom ini penting untuk filtering konten dan tampilan aplikasi, jadi harus lengkap.
"""

# Drop rows with missing values in 'Book-Author', 'Publisher', and 'Image-URL-M'
df_book.dropna(subset=['Book-Author', 'Publisher', 'Image-URL-L'], inplace=True)

# Verify the changes (optional)
print("\nMissing Values after dropping:\n", df_book.isnull().sum())
print("Number of rows after dropping missing values:", df_book.shape[0])

"""Penjelasan: Menghapus data pengguna dengan umur yang tidak wajar berdasarkan batas bawah dan atas yang telah ditentukan.

Tujuan: Umur ekstrem atau tidak masuk akal bisa bias terhadap analisis demografi pengguna.
"""

df_Users = df_Users[(df_Users['Age'] >= lower_bound) & (df_Users['Age'] <= upper_bound)]
print("\nNumber of rows after removing outliers:", len(df_Users))
print("\nAge Statistics:\n", df_Users['Age'].describe())

"""Penjelasan: Menggabungkan tabel rating dengan user dan buku untuk mendapatkan data lengkap dalam satu tabel.

Tujuan: Collaborative Filtering membutuhkan informasi pengguna, buku, dan rating dalam satu struktur data.
"""

# Gabung ratings dengan users
df_merged = df_Ratings.merge(df_Users, on='User-ID', how='left')

# Gabung hasilnya dengan books
df_merged = df_merged.merge(df_book, on='ISBN', how='left')

# Tampilkan dimensi awal
print("Jumlah data setelah merge:", df_merged.shape)

"""Penjelasan: Menghapus rating 0, yang dianggap sebagai rating implisit atau tidak sah.

Tujuan: Collaborative Filtering biasanya menggunakan rating eksplisit (yang benar-benar diberikan pengguna).
"""

# Hapus rating implisit (rating = 0)
df_merged = df_merged[df_merged['Book-Rating'] > 0]
print("Jumlah data setelah filter rating eksplisit:", df_merged.shape)

"""Penjelasan: Menyaring hanya user dan buku yang muncul lebih dari satu kali.

Tujuan: Untuk meningkatkan kualitas data, karena interaksi tunggal sulit digunakan dalam Collaborative Filtering.
"""

# Hitung frekuensi user dan buku
user_counts = df_merged['User-ID'].value_counts()
book_counts = df_merged['ISBN'].value_counts()

# Ambil hanya user dan buku yang muncul lebih dari 1 kali
df_merged = df_merged[
    df_merged['User-ID'].isin(user_counts[user_counts > 1].index) &
    df_merged['ISBN'].isin(book_counts[book_counts > 1].index)
]
print("Jumlah data setelah filter user & buku langka:", df_merged.shape)

"""Penjelasan: Membersihkan nilai usia ekstrem dan mengganti nilai kosong dengan -1 sebagai penanda "tidak diketahui".

Tujuan: Menghindari error saat analisis, dan tetap bisa mengenali pengguna yang tidak mengisi umur.
"""

# Bersihkan nilai ekstrem terlebih dahulu
df_merged['Age'] = df_merged['Age'].apply(lambda x: x if 5 <= x <= 100 else None)

# Ganti missing value Age dengan -1 (menandakan Unknown)
df_merged['Age'].fillna(-1, inplace=True)

# Cek distribusi age setelah cleaning
print(df_merged['Age'].value_counts().head())

"""Penjelasan: Mengambil hanya kolom penting untuk modeling dan mengganti nama kolom agar lebih ringkas.

Tujuan: Supaya data lebih siap dipakai dalam model Collaborative Filtering dan lebih mudah digunakan dalam coding.
"""

# Ambil hanya kolom penting
df_model = df_merged[['User-ID', 'ISBN', 'Book-Rating', 'Book-Title', 'Book-Author']].copy()

# Rename kolom agar lebih pendek
df_model.columns = ['user_id', 'isbn', 'rating', 'title', 'author']

"""# **Modeling -- Collaborative Learning**

Penjelasan:

Reader() mendefinisikan rentang nilai rating, di sini dari 1 sampai 10.

load_from_df(...) mengubah DataFrame menjadi format yang bisa dibaca oleh Surprise.

Tujuan:
Agar data bisa digunakan oleh model Surprise yang membutuhkan format dan skala rating yang eksplisit.
"""

# Format data sesuai Surprise
reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(df_model[['user_id', 'isbn', 'rating']], reader)

"""Penjelasan:

Memisahkan data menjadi 80% untuk pelatihan dan 20% untuk pengujian.

Tujuan:
Untuk menguji performa model pada data yang belum pernah dilihat sebelumnya dan mencegah overfitting.
"""

# Train-test split
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

"""Penjelasan:

SVD() membuat objek model SVD.

fit(trainset) melatih model dengan data pelatihan.

Tujuan:
SVD mempelajari pola hubungan antara pengguna dan item (buku) berdasarkan data rating, dan menghasilkan representasi laten dari preferensi.
"""

# Gunakan model SVD
model_svd = SVD()
model_svd.fit(trainset)

"""Penjelasan:

test(testset) melakukan prediksi rating terhadap data uji.

rmse(...) menghitung Root Mean Squared Error.

Tujuan:
Mengetahui seberapa jauh prediksi model dari rating asli. Semakin kecil RMSE, semakin baik modelnya.
"""

# Prediksi
predictions = model_svd.test(testset)
# Evaluasi
print("RMSE:", accuracy.rmse(predictions))

"""Penjelasan:
Mengambil ID pengguna yang paling aktif (paling banyak memberi rating).

Tujuan:
Menampilkan contoh rekomendasi personalisasi untuk pengguna yang aktif.
"""

# Pilih user
user_id = df_model['user_id'].value_counts().index[0]  # user teraktif

"""Penjelasan:

user_books: daftar buku yang sudah dibaca user.

all_books: semua ISBN unik di dataset.

books_to_predict: daftar ISBN yang belum dibaca user tersebut.

Tujuan:
Agar model merekomendasikan buku baru yang belum pernah dibaca user.


"""

# Buku yang belum pernah dia baca
user_books = df_model[df_model['user_id'] == user_id]['isbn'].tolist()
all_books = df_model['isbn'].unique()
books_to_predict = [isbn for isbn in all_books if isbn not in user_books]

"""Penjelasan:

Prediksi rating untuk setiap buku yang belum dibaca user.

Sortir berdasarkan prediksi tertinggi (.est = estimated rating).

Tujuan:
Menemukan buku mana yang kemungkinan besar akan disukai user.


"""

# Buat prediksi
predictions = [model_svd.predict(user_id, isbn) for isbn in books_to_predict]
predictions.sort(key=lambda x: x.est, reverse=True)

"""Penjelasan:

Ambil 5 buku teratas dengan rating tertinggi yang diprediksi.

Tampilkan judul, penulis, dan rating prediksi.

Tujuan:
Memberikan output rekomendasi buku personal untuk user aktif, yang bisa digunakan langsung di aplikasi rekomendasi.
"""

# Ambil top-5
top_5 = predictions[:5]
for i, pred in enumerate(top_5):
    book_info = df_model[df_model['isbn'] == pred.iid].iloc[0]
    print(f"{i+1}. {book_info['title']} by {book_info['author']} - Predicted Rating: {pred.est:.2f}")

"""# **Data Preparation untuk model Content based learning**

Penjelasan:

Membuat salinan books dan ratings untuk menjaga data asli tetap utuh.

Rename kolom books agar mudah dibaca.

Hapus baris buku yang tidak memiliki informasi penting seperti judul, penulis, atau penerbit.

Tujuan:
Menjamin data yang digunakan lengkap dan relevan untuk membuat profil konten buku (title, author, publisher).
"""

books = df_book.copy()
ratings = df_Ratings.copy()
books.columns = ['isbn', 'title', 'author', 'year', 'publisher', 'img_s', 'img_m', 'img_l']
books = books[['isbn', 'title', 'author', 'publisher', 'img_l']].copy()
books.dropna(subset=['title', 'author', 'publisher', 'img_l'], inplace=True)

"""Penjelasan:

Rename kolom ratings.

Ambil hanya data rating eksplisit (bukan nol).

Pilih hanya buku yang memiliki setidaknya satu rating eksplisit.

Tujuan:
Memastikan model hanya menggunakan buku yang benar-benar pernah dinilai oleh pengguna, sehingga bisa diasumsikan memiliki kualitas tertentu.


"""

# Filter hanya buku yang punya rating eksplisit
ratings.columns = ['user_id', 'isbn', 'rating']
explicit_ratings = ratings[ratings['rating'] > 0]
valid_isbns = explicit_ratings['isbn'].unique()
books_filtered = books[books['isbn'].isin(valid_isbns)].copy()

"""Penjelasan:

Fungsi clean_text() untuk normalisasi teks (huruf kecil dan hapus spasi tak perlu).

Menggabungkan title, author, dan publisher menjadi satu string fitur.

Tujuan:
Membuat representasi konten buku dalam satu kolom, yang akan digunakan sebagai input ke model TF-IDF untuk mencari kemiripan antar buku.
"""

# Buat fitur gabungan dari title, author, publisher
def clean_text(text):
    return str(text).lower().strip()

books_filtered['combined_features'] = (
    books_filtered['title'].apply(clean_text) + ' ' +
    books_filtered['author'].apply(clean_text) + ' ' +
    books_filtered['publisher'].apply(clean_text)
)

"""## **Modelling Content-Based Learning**

Penjelasan:

TfidfVectorizer: Mengubah teks menjadi vektor numerik berdasarkan frekuensi kata (dengan mengabaikan stopwords).

NearestNeighbors: Model untuk mencari tetangga terdekat berdasarkan kesamaan konten (menggunakan cosine similarity).

Tujuan:
Mengukur kemiripan antara buku berdasarkan kontennya, bukan rating pengguna. Cocok untuk user yang belum banyak memberi rating.
"""

# TF-IDF tetap
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(books_filtered['combined_features'])

# Nearest Neighbors model (gunakan cosine metric)
nn_model = NearestNeighbors(metric='cosine', algorithm='brute')
nn_model.fit(tfidf_matrix)

"""Penjelasan:

Reset index agar bisa diakses dengan angka.

Membuat mapping dari judul (lowercase) ke index row dalam books_filtered.

Tujuan:
Memudahkan pencarian index buku berdasarkan judul saat memanggil fungsi rekomendasi.
"""

# Mapping judul ke index
books_filtered = books_filtered.reset_index(drop=True)
title_idx = pd.Series(books_filtered.index, index=books_filtered['title'].str.lower()).drop_duplicates()

"""Penjelasan:

Terima input judul buku dari user.

Cari indexnya di title_idx.

Hitung kemiripan dengan semua buku lain berdasarkan konten gabungan.

Ambil 5 buku terdekat dengan skor kemiripan tertinggi.

Tujuan:
Menyediakan rekomendasi berdasarkan kemiripan isi buku, berguna untuk pengguna baru yang belum memberikan rating (cold-start problem).


"""

# Fungsi rekomendasi
def content_based_recommendations(title, top_n=5):
    title = title.lower()
    idx = title_idx.get(title)
    if idx is None:
        return f"Judul '{title}' tidak ditemukan."

    query_vec = tfidf_matrix[idx]
    distances, indices = nn_model.kneighbors(query_vec, n_neighbors=top_n + 1)

    results = books_filtered.iloc[indices[0][1:]][['title', 'author', 'publisher']].copy()
    results['similarity_score'] = [round(1 - dist, 4) for dist in distances[0][1:]]
    return results.reset_index(drop=True)

content_based_recommendations("See Jane Run")

content_based_recommendations("Fantastic Voyage")

"""# **Hybrid Recommendation System**

## Penjelasan setiap kode **dibawah**

Mengombinasikan kekuatan:

Content-Based Filtering → merekomendasikan buku mirip secara isi.

Collaborative Filtering (SVD) → memprediksi rating user terhadap buku berdasarkan rating pengguna lain yang mirip.

Hybrid ini membantu menangani kelemahan masing-masing metode jika digunakan sendirian, seperti cold start atau kurangnya konteks konten.

Normalisasi title, cari index-nya di data.

Ambil 10 buku yang paling mirip dari model Nearest Neighbors (TF-IDF cosine similarity).

Tambah 1 untuk menghindari diri sendiri (+1).

Tujuan: Temukan kandidat buku yang mirip berdasarkan konten untuk dijadikan bahan pertimbangan selanjutnya.

Ambil detail buku kandidat (tanpa yang pertama karena itu adalah buku input).

Hitung skor kemiripan konten (semakin dekat = skor semakin tinggi).

Tujuan: Simpan skor kemiripan sebagai metrik dari Content-Based Filtering.

Untuk setiap buku kandidat, prediksi rating dari Collaborative Filtering (SVD).

model_svd.predict(user_id, x).est mengembalikan rating yang diprediksi untuk user_id.

 Tujuan: Dapatkan prediksi seberapa besar kemungkinan user menyukai buku tersebut berdasarkan pola rating pengguna lain.

Hapus duplikat (jika ada buku dengan judul sama).

Urutkan berdasarkan hybrid_score.

Ambil rekomendasi sebanyak top_n.

Tujuan: Menghasilkan daftar rekomendasi yang paling relevan dan bernilai tinggi untuk user.
"""

def hybrid_recommendations(title, user_id, top_n=5, alpha=0.5):
    # Step 1: Content-based recommendations
    title = title.lower()
    idx = title_idx.get(title)
    if idx is None:
        return f"Judul '{title}' tidak ditemukan."

    query_vec = tfidf_matrix[idx]
    distances, indices = nn_model.kneighbors(query_vec, n_neighbors=top_n + 11)  # +1 untuk diri sendiri

    # Ambil 10 kandidat terdekat (bisa disesuaikan)
    candidates = books_filtered.iloc[indices[0][1:]][['isbn', 'title', 'author', 'publisher']].copy()
    candidates['similarity_score'] = 1 - distances[0][1:]

    # Step 2: Tambahkan prediksi dari model SVD
    candidates['predicted_rating'] = candidates['isbn'].apply(lambda x: model_svd.predict(user_id, x).est)

    # Step 3: Hitung final hybrid score
    candidates['hybrid_score'] = alpha * candidates['similarity_score'] + (1 - alpha) * candidates['predicted_rating']

    # Step 4: Sort dan ambil top-N
    results = candidates.drop_duplicates(subset='title').sort_values(by='hybrid_score', ascending=False).head(top_n)
    return results.reset_index(drop=True)

hybrid_recommendations("Harry Potter and the Goblet of Fire", user_id=11676, top_n=5, alpha=0.3)

"""Karena ini adalah model terbaik, disini saya tambahkan sedikit untuk menampilkan gambar dari book cover."""

def hybrid_recommendations(title, user_id, top_n=5, alpha=0.5):
    # Step 1: Content-based recommendations
    title = title.lower()
    idx = title_idx.get(title)
    if idx is None:
        return f"Judul '{title}' tidak ditemukan."

    query_vec = tfidf_matrix[idx]
    distances, indices = nn_model.kneighbors(query_vec, n_neighbors=top_n + 11)  # +1 untuk diri sendiri

    # Ambil 10 kandidat terdekat
    candidates = books_filtered.iloc[indices[0][1:]][['isbn', 'title', 'author', 'publisher', 'img_m']].copy()
    candidates.rename(columns={'img_m': 'image_url'}, inplace=True)  # supaya lebih generik
    candidates['similarity_score'] = 1 - distances[0][1:]

    # Step 2: Tambahkan prediksi dari model SVD
    candidates['predicted_rating'] = candidates['isbn'].apply(lambda x: model_svd.predict(user_id, x).est)

    # Step 3: Hitung final hybrid score
    candidates['hybrid_score'] = alpha * candidates['similarity_score'] + (1 - alpha) * candidates['predicted_rating']

    # Step 4: Sort dan ambil top-N
    results = candidates.drop_duplicates(subset='title').sort_values(by='hybrid_score', ascending=False).head(top_n)
    return results.reset_index(drop=True)

from IPython.display import Image, display

result_df = hybrid_recommendations("Harry Potter and the Goblet of Fire", user_id=11676, top_n=5, alpha=0.3)

for i, row in result_df.iterrows():
    print(f"{i+1}. {row['title']} by {row['author']} — Score: {row['hybrid_score']:.2f}")
    display(Image(url=row['image_url']))