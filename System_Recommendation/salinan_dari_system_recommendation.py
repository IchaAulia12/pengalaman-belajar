# -*- coding: utf-8 -*-
"""Salinan dari System_Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16u612bzDalrE7bw86y3La6oC-vC7469-
"""

!unzip '/content/drive/MyDrive/File Submission/Book_Recomendations.zip' -d Book_Recomendations

"""1. Import Library"""

import pandas as pd              # Untuk manipulasi data
import numpy as np               # Untuk operasi numerik
import matplotlib.pyplot as plt  # Untuk visualisasi (optional)
import seaborn as sns            # Visualisasi data (optional)
from sklearn.feature_extraction.text import TfidfVectorizer  # NLP - vectorizer
from sklearn.metrics.pairwise import cosine_similarity       # Hitung kemiripan antar vektor
import re                          # Untuk preprocessing teks (regex)
import string                      # Untuk hapus tanda baca
from sklearn.metrics.pairwise import linear_kernel
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy
from sklearn.neighbors import NearestNeighbors
from IPython.display import Image, display
from collections import defaultdict

"""2. Load semua dari dataset"""

df_book = pd.read_csv('/content/Book_Recomendations/Books.csv', low_memory=False)

df_Ratings = pd.read_csv('/content/Book_Recomendations/Ratings.csv')

df_Users = pd.read_csv('/content/Book_Recomendations/Users.csv')

"""# **Data Understanding**
### BOOKS
Menampilkan 5 baris pertama dari dataset
"""

df_book.head()

"""Menampilkan Jumlah baris dan kolom pada dataset Books.csv"""

print("Number of rows:", df_book.shape[0])
print("Number of columns:", df_book.shape[1])

"""Melihat Fitur yang ada pada dataset Books.csv
check tipe dari setiap data, missing value dan data duplikat.

ditemukan missing value pada:

Book-Author :           2

Publisher :             2

Image-URL-L   :         3
"""

# Display all available features (columns)
print("Available Features:\n", df_book.columns.tolist())

# Check data types of each feature
print("\nData Types:\n", df_book.dtypes)

# Check for missing values
print("\nMissing Values:\n", df_book.isnull().sum())

# Check for duplicate rows
print("\nNumber of Duplicate Rows:", df_book.duplicated().sum())

"""Melihat apakah ada outlier pada data Year-Of-Publication

Terdapat Outlier dimana buku di publish melebihi waktu sekarang dan juga sebelum tahun 1980, seperti tahun 0 atau sebelum 1500.
"""

# Calculate quartiles (Q1, Q3) and IQR
Q1 = df_book['Year-Of-Publication'].quantile(0.25)
Q3 = df_book['Year-Of-Publication'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Count outliers
outliers_low = df_book[(df_book['Year-Of-Publication'] < lower_bound)]
outliers_high = df_book[(df_book['Year-Of-Publication'] > upper_bound)]
total_outliers = len(outliers_low) + len(outliers_high)


print(f"Number of low outliers: {len(outliers_low)}")
print(f"Number of high outliers: {len(outliers_high)}")
print(f"Total number of outliers: {total_outliers}")

# Melihat apakah ada outlier pada data Year-Of-Publication
plt.figure(figsize=(10, 6))
sns.boxplot(x='Year-Of-Publication', data=df_book)
plt.title('Box Plot of Year-Of-Publication')
plt.show()

# Alternative method using describe() to identify potential outliers
print(df_book['Year-Of-Publication'].describe())

"""### **USER**

Dilakukan pengecekan juga ada pada Dataset User.
"""

df_Users.head()

"""Melihat jumlah baris data dan kolom pada dataset Users"""

print("Number of rows:", df_Users.shape[0])
print("Number of columns:", df_Users.shape[1])

"""Melihat Fitur, tipe data, missing value, dan duplikat data.

dimana disini ditemukan 110762 missing values pada data Age, yang hal ini sering ditemukan karena beberapa orang memilih tidak memberitahukan umurnya.
"""

# Display all available features (columns)
print("Available Features:\n", df_Users.columns.tolist())

# Check data types of each feature
print("\nData Types:\n", df_Users.dtypes)

# Check for missing values
print("\nMissing Values:\n", df_Users.isnull().sum())

# Check for duplicate rows
print("\nNumber of Duplicate Rows:", df_Users.duplicated().sum())

"""Melihat apakah ada Outlier pada data Age.

Disini dapat dilihat terdapat beberapa Outlier (Umur yang tidak wajar) dari min 0 dan nilai Max 244.
"""

# Check for outliers in 'Age' column of df_Users
print("\nAge Statistics:\n", df_Users['Age'].describe())

# Visualize the distribution of 'Age'
plt.figure(figsize=(10, 6))
sns.boxplot(x=df_Users['Age'])
plt.title('Boxplot of User Ages')
plt.show()

"""melihat data yang dinilai Outlier dengan IQR method"""

# Identify outliers using IQR method
Q1 = df_Users['Age'].quantile(0.25)
Q3 = df_Users['Age'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df_Users[(df_Users['Age'] < lower_bound) | (df_Users['Age'] > upper_bound)]
print("\nNumber of outliers in 'Age':", len(outliers))
print("\nOutliers:\n", outliers)

"""### **RATING**

dilakukan pengecekan yang sama pada data Rating
"""

df_Ratings.head()

"""Melihat Jumlah bari dan kolom pada dataset Ratings"""

print("Number of rows:", df_Ratings.shape[0])
print("Number of columns:", df_Ratings.shape[1])

"""Melihat semua Fitur pada dataset, Jenis tipe data, missing value dan Duplikat. yang dimana tidak ditemukan disini."""

# Display all available features (columns)
print("Available Features:\n", df_Ratings.columns.tolist())

# Check data types of each feature
print("\nData Types:\n", df_Ratings.dtypes)

# Check for missing values
print("\nMissing Values:\n", df_Ratings.isnull().sum())

# Check for duplicate rows
print("\nNumber of Duplicate Rows:", df_Ratings.duplicated().sum())

"""# **Exploratory Data Analyst (EDA)**

Melihat Publisher yang mempublish buku terbanyak pada dataset ini, yaitu :

Harlequin	7535

Silhouette	4220

Pocket	3905

Ballantine Books	3783

Bantam Books
"""

df_book['Publisher'].value_counts()

"""Melihat Author yang memiliki buku terbanyak pada dataset ini, yaitu :

Agatha Christie	632

William Shakespeare	567

Stephen King	524

Ann M. Martin	423

Carolyn Keene	373
"""

df_book['Book-Author'].value_counts()

"""Melihat Distribusi dari Ratings.

dimana disini Rating 0 adalah tidak memberikan penilaian terhadap bukunya.
"""

# Check the distribution of ratings
print(df_Ratings['Book-Rating'].describe())

# Visualize the distribution of ratings
plt.figure(figsize=(10, 6))
sns.countplot(x='Book-Rating', data=df_Ratings)
plt.title('Distribution of Book Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

"""Melihat Buku yang paling banyak di Rating oleh User, yaitu :

Wild Animus        2502.0

The Lovely Bones: A Novel        1295.0

The Da Vinci Code         883.0

Divine Secrets of the Ya-Ya Sisterhood: A Novel         732.0

The Red Tent (Bestselling Backlist)         723.0
"""

# Calculate the popularity of books based on the number of ratings
book_popularity = df_Ratings.groupby('ISBN')['Book-Rating'].count().reset_index()
book_popularity.rename(columns={'Book-Rating': 'Rating_Count'}, inplace=True)

# Merge popularity data with book information
df_merged = pd.merge(df_book, book_popularity, on='ISBN', how='left')

# Sort books by rating count in descending order and get the top 10
top_10_books = df_merged.sort_values(by='Rating_Count', ascending=False).head(10)

# Display the top 10 books with their titles and rating counts
print(top_10_books[['Book-Title', 'Rating_Count']])

"""User teraktif yang memberikan Rating, yaitu user dengan Id 11676"""

top_active_user = df_Ratings['User-ID'].value_counts().idxmax()
print(f"The most active user is: {top_active_user}")

"""Melihat Negara dengan Jumlah User terbanyak, dimana disini dapat dilihat USA menepati nomor datu, kemudian Canada dan setelahnya UK."""

# Pisahkan kolom location menjadi kota, negara bagian, negara
df_Users[['city', 'state', 'country']] = df_Users['Location'].str.split(',', expand=True, n=2)
df_Users['country'] = df_Users['country'].str.strip()  # Hilangkan spasi

# Hitung jumlah user per negara
top_countries = df_Users['country'].value_counts().head(10)

# Plot top 10 negara dengan user terbanyak
plt.figure(figsize=(10,6))
sns.barplot(x=top_countries.values, y=top_countries.index, palette='viridis')
plt.title('Top 10 Negara dengan Jumlah User Terbanyak')
plt.xlabel('Jumlah User')
plt.ylabel('Negara')
plt.tight_layout()
plt.show()

"""Disini diperlihatkan untuk Distribusi Usia pengguna.

"""

# Filter usia valid (range 5 - 100 tahun)
df_users_clean = df_Users[(df_Users['Age'] >= 5) & (df_Users['Age'] <= 100)]

# Histogram distribusi usia
plt.figure(figsize=(10,6))
sns.histplot(df_users_clean['Age'], bins=20, kde=True, color='skyblue')
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Usia')
plt.ylabel('Jumlah Pengguna')
plt.tight_layout()
plt.show()

# Tambahan: Plot distribusi kelompok usia
bins = [0, 10, 20, 30, 40, 50, 60, 70, 100]
labels = ['0-10','11-20','21-30','31-40','41-50','51-60','61-70','71+']
df_users_clean['AgeGroup'] = pd.cut(df_users_clean['Age'], bins=bins, labels=labels, right=False)

age_group_counts = df_users_clean['AgeGroup'].value_counts().sort_index()

plt.figure(figsize=(10,6))
sns.barplot(x=age_group_counts.index, y=age_group_counts.values, palette='coolwarm')
plt.title('Distribusi Pengguna Berdasarkan Kelompok Usia')
plt.xlabel('Kelompok Usia')
plt.ylabel('Jumlah Pengguna')
plt.tight_layout()
plt.show()

"""# **Data Preparation**

Penjelasan: Mengubah kolom tahun publikasi menjadi tipe numerik. Jika ada nilai tidak valid (misalnya teks), akan diubah jadi NaN.

Tujuan: Supaya bisa dianalisis atau difilter sebagai angka, bukan string.
"""

df_book['Year-Of-Publication'] = pd.to_numeric(df_book['Year-Of-Publication'], errors='coerce')
print(df_book['Year-Of-Publication'].dtype)

"""Penjelasan: Menghapus data buku dengan tahun publikasi yang tidak wajar (di luar 1980–2024).

Tujuan: Menghindari data outlier yang bisa mengganggu analisis atau rekomendasi (misalnya tahun 0 atau 9999).
"""

df_book = df_book[(df_book['Year-Of-Publication'] >= 1980) & (df_book['Year-Of-Publication'] <= 2024)]
print(df_book['Year-Of-Publication'].describe())

"""Penjelasan: Menghapus baris yang memiliki nilai kosong di kolom penting seperti penulis, penerbit, dan gambar.

Tujuan: Kolom ini penting untuk filtering konten dan tampilan aplikasi, jadi harus lengkap.
"""

# Drop rows with missing values in 'Book-Author', 'Publisher', and 'Image-URL-M'
df_book.dropna(subset=['Book-Author', 'Publisher', 'Image-URL-L'], inplace=True)

# Verify the changes (optional)
print("\nMissing Values after dropping:\n", df_book.isnull().sum())
print("Number of rows after dropping missing values:", df_book.shape[0])

"""Penjelasan: Menghapus data pengguna dengan umur yang tidak wajar berdasarkan batas bawah dan atas yang telah ditentukan.

Tujuan: Umur ekstrem atau tidak masuk akal bisa bias terhadap analisis demografi pengguna.
"""

df_Users = df_Users[(df_Users['Age'] >= lower_bound) & (df_Users['Age'] <= upper_bound)]
print("\nNumber of rows after removing outliers:", len(df_Users))
print("\nAge Statistics:\n", df_Users['Age'].describe())

"""Penjelasan: Menggabungkan tabel rating dengan user dan buku untuk mendapatkan data lengkap dalam satu tabel.

Tujuan: Collaborative Filtering membutuhkan informasi pengguna, buku, dan rating dalam satu struktur data.
"""

# Gabung ratings dengan users
df_merged = df_Ratings.merge(df_Users, on='User-ID', how='left')

# Gabung hasilnya dengan books
df_merged = df_merged.merge(df_book, on='ISBN', how='left')

# Tampilkan dimensi awal
print("Jumlah data setelah merge:", df_merged.shape)

"""Penjelasan: Menghapus rating 0, yang dianggap sebagai rating implisit atau tidak sah.

Tujuan: Collaborative Filtering biasanya menggunakan rating eksplisit (yang benar-benar diberikan pengguna).
"""

# Hapus rating implisit (rating = 0)
df_merged = df_merged[df_merged['Book-Rating'] > 0]
print("Jumlah data setelah filter rating eksplisit:", df_merged.shape)

"""Penjelasan: Menyaring hanya user dan buku yang muncul lebih dari satu kali.

Tujuan: Untuk meningkatkan kualitas data, karena interaksi tunggal sulit digunakan dalam Collaborative Filtering.
"""

# Hitung frekuensi user dan buku
user_counts = df_merged['User-ID'].value_counts()
book_counts = df_merged['ISBN'].value_counts()

# Ambil hanya user dan buku yang muncul lebih dari 1 kali
df_merged = df_merged[
    df_merged['User-ID'].isin(user_counts[user_counts > 1].index) &
    df_merged['ISBN'].isin(book_counts[book_counts > 1].index)
]
print("Jumlah data setelah filter user & buku langka:", df_merged.shape)

"""Penjelasan: Membersihkan nilai usia ekstrem dan mengganti nilai kosong dengan -1 sebagai penanda "tidak diketahui".

Tujuan: Menghindari error saat analisis, dan tetap bisa mengenali pengguna yang tidak mengisi umur.
"""

# Bersihkan nilai ekstrem terlebih dahulu
df_merged['Age'] = df_merged['Age'].apply(lambda x: x if 5 <= x <= 100 else None)

# Ganti missing value Age dengan -1 (menandakan Unknown)
df_merged['Age'].fillna(-1, inplace=True)

# Cek distribusi age setelah cleaning
print(df_merged['Age'].value_counts().head())

"""Penjelasan: Mengambil hanya kolom penting untuk modeling dan mengganti nama kolom agar lebih ringkas.

Tujuan: Supaya data lebih siap dipakai dalam model Collaborative Filtering dan lebih mudah digunakan dalam coding.
"""

# Ambil hanya kolom penting
df_model = df_merged[['User-ID', 'ISBN', 'Book-Rating', 'Book-Title', 'Book-Author']].copy()

# Rename kolom agar lebih pendek
df_model.columns = ['user_id', 'isbn', 'rating', 'title', 'author']

"""# **Modeling -- Collaborative Learning**

Penjelasan:

Reader() mendefinisikan rentang nilai rating, di sini dari 1 sampai 10.

load_from_df(...) mengubah DataFrame menjadi format yang bisa dibaca oleh Surprise.

Tujuan:
Agar data bisa digunakan oleh model Surprise yang membutuhkan format dan skala rating yang eksplisit.
"""

# Format data sesuai Surprise
reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(df_model[['user_id', 'isbn', 'rating']], reader)

"""Penjelasan:

Memisahkan data menjadi 80% untuk pelatihan dan 20% untuk pengujian.

Tujuan:
Untuk menguji performa model pada data yang belum pernah dilihat sebelumnya dan mencegah overfitting.
"""

# Train-test split
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

"""Penjelasan:

SVD() membuat objek model SVD.

fit(trainset) melatih model dengan data pelatihan.

Tujuan:
SVD mempelajari pola hubungan antara pengguna dan item (buku) berdasarkan data rating, dan menghasilkan representasi laten dari preferensi.
"""

# Gunakan model SVD
model_svd = SVD()
model_svd.fit(trainset)

"""Penjelasan:

test(testset) melakukan prediksi rating terhadap data uji.

rmse(...) menghitung Root Mean Squared Error.

Tujuan:
Mengetahui seberapa jauh prediksi model dari rating asli. Semakin kecil RMSE, semakin baik modelnya.
"""

# Prediksi
predictions = model_svd.test(testset)
# Evaluasi
print("RMSE:", accuracy.rmse(predictions))

"""Penjelasan:
Mengambil ID pengguna yang paling aktif (paling banyak memberi rating).

Tujuan:
Menampilkan contoh rekomendasi personalisasi untuk pengguna yang aktif.
"""

# Pilih user
user_id = df_model['user_id'].value_counts().index[0]  # user teraktif

"""Penjelasan:

user_books: daftar buku yang sudah dibaca user.

all_books: semua ISBN unik di dataset.

books_to_predict: daftar ISBN yang belum dibaca user tersebut.

Tujuan:
Agar model merekomendasikan buku baru yang belum pernah dibaca user.


"""

# Buku yang belum pernah dia baca
user_books = df_model[df_model['user_id'] == user_id]['isbn'].tolist()
all_books = df_model['isbn'].unique()
books_to_predict = [isbn for isbn in all_books if isbn not in user_books]

"""Penjelasan:

Prediksi rating untuk setiap buku yang belum dibaca user.

Sortir berdasarkan prediksi tertinggi (.est = estimated rating).

Tujuan:
Menemukan buku mana yang kemungkinan besar akan disukai user.


"""

# Buat prediksi
predictions = [model_svd.predict(user_id, isbn) for isbn in books_to_predict]
predictions.sort(key=lambda x: x.est, reverse=True)

"""Penjelasan:

Ambil 5 buku teratas dengan rating tertinggi yang diprediksi.

Tampilkan judul, penulis, dan rating prediksi.

Tujuan:
Memberikan output rekomendasi buku personal untuk user aktif, yang bisa digunakan langsung di aplikasi rekomendasi.
"""

# Ambil top-5
top_5 = predictions[:5]
for i, pred in enumerate(top_5):
    book_info = df_model[df_model['isbn'] == pred.iid].iloc[0]
    print(f"{i+1}. {book_info['title']} by {book_info['author']} - Predicted Rating: {pred.est:.2f}")

def get_top_n_svd(predictions, n=5):
    """
    Ambil top-N rekomendasi untuk setiap user dari prediksi SVD
    """
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = [iid for (iid, _) in user_ratings[:n]]

    return top_n

def precision_recall_at_k_svd(predictions, k=5, threshold=7.0):
    """
    Evaluasi precision@k dan recall@k berdasarkan prediksi SVD
    """
    user_est_true = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        user_est_true[uid].append((iid, est, true_r))

    precisions = []
    recalls = []

    for uid, user_ratings in user_est_true.items():
        # Relevan jika true rating >= threshold
        relevant_items = [iid for (iid, _, true_r) in user_ratings if true_r >= threshold]
        recommended_items = [iid for (iid, est, _) in sorted(user_ratings, key=lambda x: x[1], reverse=True)[:k]]

        if not relevant_items:
            continue

        hit_set = set(recommended_items) & set(relevant_items)
        precision = len(hit_set) / k
        recall = len(hit_set) / len(relevant_items)

        precisions.append(precision)
        recalls.append(recall)

    return sum(precisions) / len(precisions), sum(recalls) / len(recalls)

# Prediksi dari model SVD (gunakan model_svd yang telah dilatih sebelumnya)
predictions = model_svd.test(testset)
top_n_svd = get_top_n_svd(predictions, n=5)
precision_svd, recall_svd = precision_recall_at_k_svd(predictions, k=5)

print(f"[SVD] Precision@5: {precision_svd:.4f}")
print(f"[SVD] Recall@5: {recall_svd:.4f}")

"""# **Data Preparation untuk model Content based learning**

Penjelasan:

Membuat salinan books dan ratings untuk menjaga data asli tetap utuh.

Rename kolom books agar mudah dibaca.

Hapus baris buku yang tidak memiliki informasi penting seperti judul, penulis, atau penerbit.

Tujuan:
Menjamin data yang digunakan lengkap dan relevan untuk membuat profil konten buku (title, author, publisher).
"""

books = df_book.copy()
ratings = df_Ratings.copy()

"""Ganti nama kolom agar lebih mudah dipanggil.
Mempermudah pengolahan data di tahapan selanjutnya dan hanya ambil kolom yang relevan untuk rekomendasi (bukan ukuran gambar kecil).
"""

# Rename kolom agar lebih mudah dibaca
books.columns = ['isbn', 'title', 'author', 'year', 'publisher', 'img_s', 'img_m', 'img_l']
books = books[['isbn', 'title', 'author', 'publisher', 'img_l']].copy()

# Hapus data yang kosong di kolom penting
books.dropna(subset=['title', 'author', 'publisher', 'img_l'], inplace=True)

"""Gabungkan informasi teks penting jadi satu string.
Ini adalah fitur masukan untuk TF-IDF, yang akan digunakan oleh model content-based untuk mengukur kesamaan antar buku.
"""

# Gabungkan fitur penting menjadi satu string per buku
books['combined_features'] = (
    books['title'].fillna('') + ' ' +
    books['author'].fillna('') + ' ' +
    books['publisher'].fillna('')
)

"""- Membersihkan rating yang tidak lengkap.

- Menyaring hanya buku yang masih ada di books.

- Tambahan user_id di akhir tampaknya keliru karena user_id = ISBN, ini tampaknya bug.

Membersihkan data agar evaluasi akurat dan hanya mencakup item valid yang bisa direkomendasikan.
"""

RELEVANT_THRESHOLD = 7

# Filter rating yang valid
ratings = ratings.dropna(subset=['ISBN', 'User-ID', 'Book-Rating'])
ratings = ratings[ratings['ISBN'].isin(books['isbn'])]
ratings = ratings[ratings['ISBN'].notnull()]
ratings['user_id'] = ratings['ISBN'].astype(str)
#ratings.rename(columns={'User-ID': 'user_id', 'ISBN': 'isbn', 'Book-Rating': 'rating'}, inplace=True)

"""disini diberi batasan dikarenakan saat percobaan, google colab selalu mengulang runtime diikarenakan pembatasan memori, sehingga untuk percobaan diambil 5000 sample.
Mengubah teks (title, author, publisher) menjadi vektor numerik.Ini inti dari content-based filtering — menemukan buku serupa berdasarkan fitur deskripsi.
"""

# Vectorizer dengan batasan memori: hanya kata penting dan unigram saja
tfidf = TfidfVectorizer(
    stop_words='english',
    max_features=5000,  # Batas atas fitur agar hemat RAM
    strip_accents='unicode',
    lowercase=True
)

tfidf_matrix = tfidf.fit_transform(books['combined_features'])

"""dikarenakan disini menggunakan Content-based learning, dan memang dataset tidak memiliki fitur yang mendukung untuk modelling content-based learning membuat model sensitive terhadap judul dari buku sehingga Membuat versi bersih dari judul agar pencarian judul lebih akurat (tanpa tanda baca)."""

# Membuat indeks berdasarkan title yang dibersihkan
books['clean_title'] = books['title'].str.lower().str.replace('[^a-z0-9 ]', '', regex=True)
title_to_index = pd.Series(books.index, index=books['clean_title'])

"""## **Modelling Content-Based Learning**

Melatih model untuk menemukan buku yang paling mirip secara konten.
Algoritma inti dari content-based filtering – menemukan tetangga terdekat dari input.
"""

nn_model = NearestNeighbors(metric='cosine', algorithm='brute')
nn_model.fit(tfidf_matrix)

""" Cari top-n buku yang mirip berdasarkan input judul."""

def get_recommendations(title, books, tfidf_matrix, nn_model, top_n=10):
    idx = books[books['title'] == title].index
    if len(idx) == 0:
        return None
    idx = idx[0]
    tfidf_vec = tfidf_matrix[idx]
    distances, indices = nn_model.kneighbors(tfidf_vec, n_neighbors=top_n+1)
    result_indices = indices[0][1:]  # skip the input itself
    return books.iloc[result_indices]

recommended_books = get_recommendations("Harry Potter and the Sorcerer's Stone", books, tfidf_matrix, nn_model)
recommended_books[['title', 'author', 'publisher', 'img_l']]

"""Evaluasi kualitas rekomendasi dengan membandingkan hasil model terhadap data user yang nyata.
- Untuk setiap user, ambil 1 buku relevan sebagai query.

- Hitung berapa dari top-k hasil rekomendasi yang juga disukai user.
"""

def precision_recall_at_k_nn(books, ratings, tfidf_matrix, nn_model, k=5, n_users=100, RELEVANT_THRESHOLD=8):
    hits = 0
    total_recommended = 0
    total_relevant = 0

    user_grouped = ratings.groupby('User-ID')  # sesuai struktur asli
    evaluated_users = 0

    for user_id, group in user_grouped:
        user_ratings = group.sort_values(by='Book-Rating', ascending=False)
        relevant_books = user_ratings[user_ratings['Book-Rating'] >= RELEVANT_THRESHOLD]['ISBN'].tolist()

        if len(relevant_books) < 2:
            continue

        query_book = relevant_books[0]
        query_title = books[books['isbn'] == query_book]['title'].values
        if len(query_title) == 0:
            continue

        title = query_title[0]
        recommended_df = get_recommendations(title, books, tfidf_matrix, nn_model, top_n=k)
        if recommended_df is None:
            continue

        recommended_isbns = books.loc[recommended_df.index]['isbn'].tolist()
        relevant_set = set(relevant_books[1:])  # exclude query book

        hit_set = set(recommended_isbns) & relevant_set
        hits += len(hit_set)
        total_recommended += k
        total_relevant += len(relevant_set)

        evaluated_users += 1
        if evaluated_users >= n_users:
            break

    precision = hits / total_recommended if total_recommended > 0 else 0
    recall = hits / total_relevant if total_relevant > 0 else 0
    return precision, recall

"""Melihat seberapa akurat dan lengkap sistem dalam memberikan rekomendasi.

- Precision@5 → proporsi buku dari hasil rekomendasi yang memang disukai.

- Recall@5 → proporsi buku yang disukai user dan berhasil direkomendasikan.


"""

precision, recall = precision_recall_at_k_nn(books, ratings, tfidf_matrix, nn_model, k=5, n_users=100)
print(f"Precision@5: {precision:.4f}")
print(f"Recall@5: {recall:.4f}")